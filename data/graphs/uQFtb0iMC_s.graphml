<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d11" for="edge" attr.name="type" attr.type="string" />
  <key id="d10" for="edge" attr.name="seq" attr.type="string" />
  <key id="d9" for="edge" attr.name="notes" attr.type="string" />
  <key id="d8" for="edge" attr.name="flow_id" attr.type="long" />
  <key id="d7" for="node" attr.name="notes" attr.type="string" />
  <key id="d6" for="node" attr.name="service" attr.type="string" />
  <key id="d5" for="node" attr.name="name" attr.type="string" />
  <key id="d4" for="graph" attr.name="graph_usable" attr.type="boolean" />
  <key id="d3" for="graph" attr.name="categories" attr.type="string" />
  <key id="d2" for="graph" attr.name="notes" attr.type="string" />
  <key id="d1" for="graph" attr.name="link" attr.type="string" />
  <key id="d0" for="graph" attr.name="name" attr.type="string" />
  <graph edgedefault="directed">
    <node id="0">
      <data key="d5" />
      <data key="d6">S3</data>
      <data key="d7">DATA_PEEK: multimedia (audio/video/images)</data>
    </node>
    <node id="1">
      <data key="d5" />
      <data key="d6">Lambda</data>
      <data key="d7">WORKLOAD_PEEK: triggers step function</data>
    </node>
    <node id="2">
      <data key="d5" />
      <data key="d6">StepFunctions</data>
      <data key="d7">WORKLOAD_PEEK: triggers different APIs based on media type</data>
    </node>
    <node id="3">
      <data key="d5" />
      <data key="d6">MediaConvert</data>
      <data key="d7">WORKLOAD_PEEK: extract audio from video.</data>
    </node>
    <node id="4">
      <data key="d5" />
      <data key="d6">Rekognition</data>
      <data key="d7" />
    </node>
    <node id="5">
      <data key="d5" />
      <data key="d6">Transcribe</data>
      <data key="d7" />
    </node>
    <node id="6">
      <data key="d5" />
      <data key="d6">Comprehend</data>
      <data key="d7" />
    </node>
    <node id="7">
      <data key="d5" />
      <data key="d6">OpenSearch</data>
      <data key="d7" />
    </node>
    <edge source="0" target="1" id="0">
      <data key="d8">0</data>
      <data key="d9" />
      <data key="d10">0</data>
      <data key="d11">data</data>
    </edge>
    <edge source="1" target="2" id="0">
      <data key="d8">0</data>
      <data key="d9" />
      <data key="d10">1</data>
      <data key="d11">data</data>
    </edge>
    <edge source="2" target="4" id="0">
      <data key="d8">1</data>
      <data key="d9" />
      <data key="d10">0</data>
      <data key="d11">data</data>
    </edge>
    <edge source="2" target="3" id="0">
      <data key="d8">2</data>
      <data key="d9" />
      <data key="d10">0</data>
      <data key="d11">data</data>
    </edge>
    <edge source="3" target="5" id="0">
      <data key="d8">2</data>
      <data key="d9" />
      <data key="d10">1</data>
      <data key="d11">data</data>
    </edge>
    <edge source="4" target="7" id="0">
      <data key="d8">1</data>
      <data key="d9" />
      <data key="d10">1</data>
      <data key="d11">data</data>
    </edge>
    <edge source="5" target="7" id="0">
      <data key="d8">2</data>
      <data key="d9" />
      <data key="d10">2</data>
      <data key="d11">data</data>
    </edge>
    <edge source="5" target="6" id="0">
      <data key="d8">3</data>
      <data key="d9" />
      <data key="d10">0</data>
      <data key="d11">data</data>
    </edge>
    <edge source="6" target="7" id="0">
      <data key="d8">3</data>
      <data key="d9" />
      <data key="d10">1</data>
      <data key="d11">data</data>
    </edge>
    <data key="d0">AWS Solutions: Media Analysis</data>
    <data key="d1">https://www.youtube.com/watch?v=uQFtb0iMC_s</data>
    <data key="d2">why step functions? a) Allows to run multiple AI services based on media type.  b) Extensibility for customer. 'exactly why Step Functions was used': developer can add a branch e.g. send to SageMaker/Textract and send to elasticsearch.
    HACK &amp; LIMITATION: Transcribe has limitation of audio size. Use MediaConvert to extract audio from video to bypass this limitation.
    any persistence layer is fine (no particular need for opensearch/elasticsearch), e.g., since the output is JSON, could be stored in S3 and query with Athena.</data>
    <data key="d3">data_ingestion</data>
    <data key="d4">True</data>
  </graph>
</graphml>
